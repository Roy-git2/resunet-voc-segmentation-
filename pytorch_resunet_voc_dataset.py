# -*- coding: utf-8 -*-
"""Pytorch-ResUNet-VOC_Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/roysrikrishnaanalyst/pytorch-resunet-voc-dataset.ae95126c-69d0-461b-891f-9515e8424b76.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250628/auto/storage/goog4_request%26X-Goog-Date%3D20250628T092610Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8d4d5e9d329acdb3b4bad339672f56b53a80e0e5c5ddfecf8cc4ac1e928f1b841830e8bf1a58285a79797d86608c342e53c2ff797788f5079d2a8f299f76427fda9e6aeb15f45601d2cb12da9aa8ec8275575e55f72842af2eea2bd57902b638012ac19594eb3a83eee1d5010b17c50865d03649a1a50fc0b3642bb6164c75b8a42387f7b166642a89b9b193c8e62eb9a8b95ac3cd4886efe11f63ef2abd69ab5d947fbd315824b5d4c1e83e3038e7ef59be4c0800269f8042376bd549b3286c73ed1ce5625a4e0130a755d435ca98695879e93f7455809d060415fd7d969dd836e0a575fe673f15d7cdba69223b508aef318983038f5e32143bf5edbd7fa231
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
zaraks_pascal_voc_2007_path = kagglehub.dataset_download('zaraks/pascal-voc-2007')
huanghanchina_pascal_voc_2012_path = kagglehub.dataset_download('huanghanchina/pascal-voc-2012')

print('Data source import complete.')

"""<h1 align="center" style="color: rgb(51, 204, 255); font-weight: bold; font-size: 38px">
<br>
  PyTorch ResUNet with the VOC Datase Ⓐ
</h1>

<h1 align="center">
<img src="https://github.com/user-attachments/assets/f9533713-60fc-4fce-8080-a7b66dbfc36e" alt="ReadMe" width="700">

<img src="https://github.com/user-attachments/assets/a729c62f-b909-4c85-b0cf-c51809e05e01" alt="ReadMe" width="500">
</h1>
"""

!pip install -U torchmetrics


# !pip install torchmetrics==0.11.4

import torchmetrics
print(torchmetrics.__version__)


# !pip uninstall -y torchmetrics
# !pip install torchmetrics==1.7.3

import torchmetrics.segmentation as tm_seg
print(dir(tm_seg))

"""## 1- Import Library"""

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Compose, ToTensor, Resize
import torchmetrics
from torchmetrics.classification import BinaryAccuracy
import torch.optim as optim
from torch import optim, nn
from torchmetrics.segmentation import GeneralizedDiceScore, MeanIoU
from torchmetrics import  JaccardIndex

from torchmetrics.segmentation import DiceScore, GeneralizedDiceScore, MeanIoU

from torchvision.datasets import VOCSegmentation

import timm
import albumentations as A
from albumentations.pytorch import ToTensorV2 # np.array -> torch.tensor

import numpy as np
import cv2
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
from glob import glob
import argparse
import time
import warnings
from PIL import Image
warnings.filterwarnings("ignore")

"""## 2- Custom Dataset For Albumentation

### 2.1- Custom dataset
"""

cv2.setNumThreads(0)
cv2.ocl.setUseOpenCL(False)

VOC_CLASSES = [
    "background",
    "aeroplane",
    "bicycle",
    "bird",
    "boat",
    "bottle",
    "bus",
    "car",
    "cat",
    "chair",
    "cow",
    "diningtable",
    "dog",
    "horse",
    "motorbike",
    "person",
    "potted plant",
    "sheep",
    "sofa",
    "train",
    "tv/monitor",
]

VOC_COLORMAP = [
    [0, 0, 0],
    [128, 0, 0],
    [0, 128, 0],
    [128, 128, 0],
    [0, 0, 128],
    [128, 0, 128],
    [0, 128, 128],
    [128, 128, 128],
    [64, 0, 0],
    [192, 0, 0],
    [64, 128, 0],
    [192, 128, 0],
    [64, 0, 128],
    [192, 0, 128],
    [64, 128, 128],
    [192, 128, 128],
    [0, 64, 0],
    [128, 64, 0],
    [0, 192, 0],
    [128, 192, 0],
    [0, 64, 128],
]

class PascalVOCSearchDataset(VOCSegmentation):
    def __init__(self, root="~/data/pascal_voc", image_set="train", download=True, transform=None):
        super().__init__(root=root, image_set=image_set, download=download, transform=transform)

    @staticmethod
    def _convert_to_segmentation_mask(mask):
        # This function converts a mask from the Pascal VOC format to the format required by AutoAlbument.
        #
        # Pascal VOC uses an RGB image to encode the segmentation mask for that image. RGB values of a pixel
        # encode the pixel's class.
        #
        # AutoAlbument requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes].
        # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have
        # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise.
        height, width = mask.shape[:2]
        segmentation_mask = np.zeros((height, width, len(VOC_COLORMAP)), dtype=np.float32)
        for label_index, label in enumerate(VOC_COLORMAP):
            segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)
        return segmentation_mask #0, 1, 2, 3, ..., 20 (H, W, C) -> (H, W, 1) -> (H, W) #numpy

    def __getitem__(self, index):
        image = cv2.imread(self.images[index])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(self.masks[index])
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)
        mask = self._convert_to_segmentation_mask(mask)
        if self.transform is not None:
            transformed = self.transform(image=image, mask=mask)
            image = transformed["image"]
            mask = transformed["mask"].argmax(dim=2).squeeze()
        return image, mask

"""### 2.2- Test dataset"""

# helper function for data visualization
def visualize(**images):
    """PLot images in one row."""
    n = len(images)
    plt.figure(figsize=(16, 5))
    for i, (name, image) in enumerate(images.items()):
        plt.subplot(1, n, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.title(' '.join(name.split('_')).title())
        plt.imshow(image)
    plt.show()

trainsform = A.Compose([
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
])

VOC_dataset = PascalVOCSearchDataset(download=True,image_set= 'val', transform = trainsform)

idx = np.random.randint(1,100)
img, mask = VOC_dataset[idx]
print("Img:", img.shape)
print("Mask:", mask.shape)
print("Max Value Img:",img.max())
print("Max Value Mask",mask.max())

visualize(
    image=img.permute(1,2,0),
    mask=mask.squeeze(),
)

"""## 3- Build ResUnet Model

### 3.1- ResUNet Parts
"""

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        """
        Double convolution layer with ReLU activation.

        Parameters:
        - in_channels: int, number of input channels.
        - out_channels: int, number of output channels.
        """
        super().__init__()
        self.conv_op = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv_op(x)

class DownSample(nn.Module):
    def __init__(self):
        """
        Downsampling layer with double convolution and max pooling.

        Parameters:
        - in_channels: int, number of input channels.
        - out_channels: int, number of output channels.
        """
        super().__init__()
        self.backbone = timm.create_model("resnet50", pretrained=True, features_only=True)

    def forward(self, x):
        down1, down2, down3, down4, down5= self.backbone(x)
        return down1, down2, down3, down4, down5

class UpSample(nn.Module):
    def __init__(self, in_channels, out_channels):
        """
        Upsampling layer with transpose convolution and double convolution.

        Parameters:
        - in_channels: int, number of input channels.
        - out_channels: int, number of output channels.
        """
        super().__init__()
        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
        self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        x = torch.cat([x1, x2], 1)
        return self.conv(x)

"""### 3.2- ResUnet Model"""

import torch.nn as nn

class ResUNet(nn.Module):
    def __init__(self, in_channels, num_classes):
        """
        UNet architecture for image segmentation.

        Parameters:
        - in_channels: int, number of input channels.
        - num_classes: int, number of output classes.
        """
        super().__init__()

        # Down-sampling path
        self.down_convolution = DownSample()

        # Bottleneck layer
        self.bottle_neck = DoubleConv(2048, 2048)

        # Up-sampling path
        self.up_convolution_1 = UpSample(2048, 1024)
        self.up_convolution_2 = UpSample(1024, 512)
        self.up_convolution_3 = UpSample(512, 256)
        self.up_convolution_3_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)
        self.up_convolution_4 = UpSample(128, 64)

        # Output layer
        self.out = nn.Sequential(
            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),  # Upsample
            nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)  # Final conv layer
        )

    def forward(self, x):
        # Down-sampling path
        down_1, down_2, down_3, down_4, down_5 = self.down_convolution(x)

        # Bottleneck
        b = self.bottle_neck(down_5)

        # Up-sampling with skip connections
        up_1 = self.up_convolution_1(b, down_4)
        up_2 = self.up_convolution_2(up_1, down_3)
        up_3 = self.up_convolution_3(up_2, down_2)
        up_3_1 = self.up_convolution_3_1(up_3)
        up_4 = self.up_convolution_4(up_3_1, down_1)

        # Output layer
        out = self.out(up_4)
        return out

"""## 4- Train ResUNet Model

### 4.1- Define Transform Function
"""

#Define trainform to images
def transform():
    train_transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),
        A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),
        A.RandomCrop(height=320, width=320, always_apply=True),
        A.GaussNoise(p=0.2),
        A.Perspective(p=0.5),
        A.OneOf(
            [A.CLAHE(p=1),
            A.RandomBrightnessContrast(p=1),
            A.RandomGamma(p=1)],
            p=0.9,
        ),
        A.Resize(width=256, height=256),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),
        ToTensorV2(),
    ])

    test_transform = A.Compose([
        A.Resize(width=256, height=256),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),
        ToTensorV2(),
    ])
    return train_transform, test_transform

"""### 4.2- Define Training-Eval Function"""

# Training fuction
def train(model,train_dataloader,device,optimizer, criterion, epoch, EPOCHS):
    model.train()
    train_progress= tqdm(train_dataloader, colour="cyan")
    for idx, img_mask in enumerate(train_progress):
        img = img_mask[0].float().to(device) #img - B,C,H,W
        mask = img_mask[1].long().to(device) #label - B,H,W

        y_pred = model(img) #B,21, H, W

        #Optimizer
        optimizer.zero_grad()
        loss = criterion(y_pred, mask)
        loss.backward()
        optimizer.step()

        train_progress.set_description("TRAIN| Epoch: {}/{}| Iter: {}/{} | Loss: {:0.4f}".format(
            epoch, EPOCHS, idx, len(train_dataloader), loss))

#Evaluate function
def evaluate(model,val_dataloader,device, criterion, miou_metric, dice_metric):
    model.eval()
    #list metrics
    all_loss = []
    all_mIOU = []
    all_dice = []

    with torch.no_grad():
        for idx, img_mask in enumerate(val_dataloader):
            img = img_mask[0].float().to(device)
            mask = img_mask[1].long().to(device) #B W H

            y_pred = model(img) #B 21 H W

            loss = criterion(y_pred, mask)

            # (B, 21, H, W) -> (B, 1, H, W) -> (B, H, W)
            y_pred = y_pred.argmax(dim=1).squeeze(dim=1)
            miou = miou_metric(y_pred, mask)
            dice = dice_metric(y_pred, mask)

            all_loss.append(loss.cpu().item())
            all_mIOU.append(miou.cpu().item())
            all_dice.append(dice.cpu().item())
            if idx > 30: break

    # Compute metrics for the epoch
    loss = np.mean(all_loss)
    miou = np.mean(all_mIOU)
    dice = np.mean(all_dice)

    print("VAL| Loss: {:0.4f} | IOU: {:0.4f} | Dice: {:0.4f}".format(
            loss, miou, dice))

    return dice

"""### 4.3- Start Traning Model"""

def main():
    # Hyperparameters and paths
    LEARNING_RATE = 0.0001
    BATCH_SIZE = 10
    EPOCHS = 81
    NUM_WORKERS = 4
    DATA_PATH = "data"
    MODEL_SAVE_PATH = "/content/drive/"
    NUM_CLASSES = len(VOC_CLASSES)

    # Create model save directory if it doesn't exist
    if not os.path.isdir(MODEL_SAVE_PATH):
        os.mkdir(MODEL_SAVE_PATH)

    # Data augmentation and preprocessing for training and testing
    train_transform, test_transform = transform()

    # Create datasets and dataloaders
    train_dataset = PascalVOCSearchDataset(image_set="train", transform=train_transform, download=False)
    val_dataset = PascalVOCSearchDataset(image_set="val", transform=test_transform, download=False)

    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=True)
    val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, drop_last=True)

    # Load model and move it to the appropriate device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    #checkpoint = torch.load("/kaggle/working/models/last.pt")
    model = ResUNet(in_channels=3, num_classes=NUM_CLASSES)
    #model.load_state_dict(checkpoint['model_state_dict'])
    # Freeze part of model
    freeze_layers = ['down_convolution']
    for name, param in model.named_parameters():
        if any(layer in name for layer in freeze_layers):
            param.requires_grad = False

    model.to(device)

    # Initialize optimizer and loss function
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    criterion = nn.CrossEntropyLoss()

    # Metrics
    # dice_metric = Dice(num_classes=NUM_CLASSES, average="macro").to(device)
    from torchmetrics.segmentation import DiceScore

    dice_metric = DiceScore(num_classes=NUM_CLASSES, average="macro").to(device)

    miou_metric = JaccardIndex(num_classes=NUM_CLASSES, task="multiclass", average="macro").to(device)
    # Best validation IoU for saving the best model
    best_predict = -1
    #best_predict = checkpoint['dice']
    current_epoch = 0
    #current_epoch = checkpoint['epoch']
    # Training loop
    for epoch in range(current_epoch,EPOCHS): #EPOCHS
        train(model,train_dataloader,device,optimizer, criterion, epoch, EPOCHS)
        dice= evaluate(model,val_dataloader,device, criterion, miou_metric, dice_metric)

        #Create checkpoint
        checkpoint = {
            "model_state_dict": model.state_dict(),
            "epoch": epoch,
            "optimizer_state_dict": optimizer.state_dict(),
            "dice": dice
        }

        # Save last checkpoint
        torch.save(checkpoint, os.path.join(MODEL_SAVE_PATH, "last.pt"))

        # Save best checkpoint based on dice score
        if dice > best_predict:
            torch.save(checkpoint, os.path.join(MODEL_SAVE_PATH, "best.pt"))
            best_predict = dice

from google.colab import drive
drive.mount('/content/drive')

if __name__ == '__main__':
    main()
    # pass

"""![Progess_bar](https://github.com/user-attachments/assets/e6e5ff44-956f-4a9b-bf57-88802a327099)

#### Results after training for 80 epochs - Learning Rate: 1e-4 :¶
* Loss: 0.39
* mIOU: 0.4
* Dice: 0.48

## 5- Inference ResUnet

### 5.1- Unnormalize Images
"""

#Unnormalize image
class UnNormalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        """
        Args:
            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.
        Returns:
            Tensor: Normalized image.
        """
        for t, m, s in zip(tensor, self.mean, self.std):
            t.mul_(s).add_(m)
            # The normalize code -> t.sub_(m).div_(s)
        return tensor

unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))

"""### 5.2- Show_images Fuction"""

def pred_show_image_grid(model_pth, device, transform, num_classes):
    # Load model checkpoint
    checkpoint = torch.load(model_pth)

    # Initialize and load model state
    model = ResUNet(in_channels=3, num_classes=num_classes)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)

    # Load validation dataset
    image_dataset = PascalVOCSearchDataset(image_set="val", transform=test_transform, download=False)

    images, orig_masks, pred_masks = [], [], []

    for _ in range(6):
        # Randomly select an image and mask
        idx = np.random.randint(len(image_dataset))
        img, orig_mask = image_dataset[idx]
        # Store original image
        images.append(unorm(img).permute(1, 2, 0))

        # Predict mask
        img = img.float().to(device).unsqueeze(0)
        pred_mask = model(img).argmax(dim=1).squeeze(0).cpu().numpy()

        # Colorize masks
        color_pred_mask = np.zeros((*pred_mask.shape, 3))
        color_mask = np.zeros((*orig_mask.shape, 3))
        for i, color in enumerate(VOC_COLORMAP):
            color_pred_mask[pred_mask == i] = np.array(color)
            color_mask[orig_mask == i] = np.array(color)

        orig_masks.append(color_mask)
        pred_masks.append(color_pred_mask)

    # Combine images and masks for display
    images.extend(orig_masks)
    images.extend(pred_masks)

    # Plot images and masks
    fig = plt.figure(figsize=(15, 8))
    for i in range(1, 3*6 + 1):
        fig.add_subplot(3, 6, i)
        plt.imshow(images[i - 1])

"""### 5.3- Show images"""

# Define the transformation to be applied to the test images
test_transform = A.Compose([
    A.Resize(width=256, height=256),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),
    ToTensorV2(),
])
#Note: You must train the model before using this param
MODEL_PATH = "/kaggle/working/models/best.pt"
NUM_CLASSES=  len(VOC_CLASSES)
device = "cuda" if torch.cuda.is_available() else "cpu"
#Show images
# pred_show_image_grid(MODEL_PATH, device, test_transform, NUM_CLASSES)

"""#### After 20 epochs:

![Demo](https://github.com/user-attachments/assets/c37cf360-844e-4f2c-b9e2-0c9fa5645ac8)

#### After 80 epochs:

![Demo](https://github.com/user-attachments/assets/42e91e76-ef6e-4527-82f5-823d9e7d5c53)
"""